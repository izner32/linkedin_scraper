{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIND USER'S POSTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for scraping \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# misc\n",
    "import re as re # regex \n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# accesing env file \n",
    "import os \n",
    "from dotenv import load_dotenv # to access the secret keys we've hidden in a separate file \n",
    "load_dotenv() # grab values inside env file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selenium and LinkedIn Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.getenv(\"WEBDRIVER_PATH\")\n",
    "USERNAME = os.getenv(\"LI_USERNAME\")\n",
    "PASSWORD = os.getenv(\"LI_PASS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize web driver that would control the web browser\n",
    "ser = Service(PATH)\n",
    "op = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=ser, options=op)\n",
    "\n",
    "# website we wanted to access \n",
    "driver.get(\"https://www.linkedin.com/uas/login\")\n",
    "time.sleep(3) # added a pause to avoid getting marked as bot \n",
    "\n",
    "# login in linkedin\n",
    "email=driver.find_element(By.ID,\"username\")\n",
    "email.send_keys(USERNAME)\n",
    "password=driver.find_element(By.ID,\"password\")\n",
    "password.send_keys(PASSWORD)\n",
    "time.sleep(3)\n",
    "password.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Scraping Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lists\n",
    "account_links = [\"https://www.linkedin.com/in/dkjapan/\",\"https://www.linkedin.com/in/vidsrinivasan/\"] # sample linkedin profiles\n",
    "post_texts = []\n",
    "post_names = []\n",
    "\n",
    "def user_post_scrape(account_link):\n",
    "    name = account_link[28:-1]\n",
    "    time.sleep(10)\n",
    "\n",
    "    driver.get(account_link + 'detail/recent-activity/shares/')  \n",
    "    start=time.time()\n",
    "    lastHeight = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "        newHeight = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if newHeight == lastHeight:\n",
    "            break\n",
    "        lastHeight = newHeight\n",
    "        end=time.time()\n",
    "        if round(end-start)>20: # scrolling thru website - pretending we're not a bot for 20 seconds \n",
    "            break\n",
    "\n",
    "    company_page = driver.page_source   \n",
    "\n",
    "    linkedin_soup = bs(company_page.encode(\"utf-8\"), \"html\")\n",
    "    linkedin_soup.prettify()\n",
    "    containers = linkedin_soup.findAll(\"div\",{\"class\":\"occludable-update ember-view\"})\n",
    "    print(\"Fetching data from account: \"+ name)\n",
    "    \n",
    "    iterations = 0\n",
    "    nos = 1 # enter number of posts\n",
    "    for container in containers:\n",
    "        try:\n",
    "            text_box = container.find(\"div\",{\"class\":\"feed-shared-update-v2__commentary\"})\n",
    "            text = text_box.find(\"span\",{\"dir\":\"ltr\"})\n",
    "            post_texts.append(text.get_text())\n",
    "            post_names.append(name)\n",
    "            iterations += 1\n",
    "            \n",
    "            if(iterations==nos):\n",
    "                break\n",
    "\n",
    "        except:\n",
    "            print(\"There is an error fetching user's post. Initiating exit.\")\n",
    "            driver.quit()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from account: dkjapan\n",
      "Fetching data from account: vidsrinivasan\n"
     ]
    }
   ],
   "source": [
    "# Using loop to execute scraping \n",
    "n = int(len(account_links))\n",
    "for j in range(n):\n",
    "    user_post_scrape(account_links[j])\n",
    "\n",
    "# Quit \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing scraped data into specified file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Usernames\": post_names,\n",
    "    \"Posts\": post_texts,\n",
    "}\n",
    "\n",
    "# save to csv \n",
    "df = pd.DataFrame(data)\n",
    "df.head(5)\n",
    "df.to_csv(\"user_posts.csv\", encoding='utf-8', index=False)\n",
    "\n",
    "# save to excel \n",
    "# writer = pd.ExcelWriter(\"user_posts.xlsx\", engine='xlsxwriter')\n",
    "# df.to_excel(writer, index =False)\n",
    "# writer.save()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bfb4883d108fc92ac768439090a2e92bb9a1f760a54beeecfd6762b5dcd70fe3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
