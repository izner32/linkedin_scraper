{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for scraping \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# misc\n",
    "import re as re # regex \n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# accesing env file \n",
    "import os \n",
    "from dotenv import load_dotenv, find_dotenv # to access the secret keys we've hidden in a separate file \n",
    "load_dotenv(find_dotenv()) # grab values inside env file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selenium and LinkedIn Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = os.getenv(\"WEBDRIVER_PATH\")\n",
    "USERNAME = os.getenv(\"AWS_KEY\")\n",
    "PASSWORD = os.getenv(\"AWS_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize web driver that would control the web browser\n",
    "driver = webdriver.Chrome(PATH)\n",
    "\n",
    "# website we wanted to access \n",
    "driver.get(\"https://www.linkedin.com/uas/login\")\n",
    "time.sleep(3) # added a pause to avoid getting marked as bot \n",
    "\n",
    "# login in linkedin\n",
    "email=driver.find_element_by_id(\"username\")\n",
    "email.send_keys(USERNAME)\n",
    "password=driver.find_element_by_id(\"password\")\n",
    "password.send_keys(PASSWORD)\n",
    "time.sleep(3)\n",
    "password.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Scraping Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape_func(post_links,post_texts,post_names):\n",
    "    name = post_links[28:-1]\n",
    "    page = post_linksa\n",
    "    time.sleep(10)\n",
    "\n",
    "    driver.get(page + 'detail/recent-activity/shares/')  \n",
    "    start=time.time()\n",
    "    lastHeight = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "        newHeight = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if newHeight == lastHeight:\n",
    "            break\n",
    "        lastHeight = newHeight\n",
    "        end=time.time()\n",
    "        if round(end-start)>20: # how long the webdriver gets to collect posts, in this case 20 seconds \n",
    "            break\n",
    "\n",
    "    company_page = driver.page_source   \n",
    "\n",
    "    linkedin_soup = bs(company_page.encode(\"utf-8\"), \"html\")\n",
    "    linkedin_soup.prettify()\n",
    "    containers = linkedin_soup.findAll(\"div\",{\"class\":\"occludable-update ember-view\"})\n",
    "    print(\"Fetching data from account: \"+ name)\n",
    "    iterations = 0\n",
    "    nos = int(input(\"Enter number of posts: \"))\n",
    "    for container in containers:\n",
    "\n",
    "        try:\n",
    "            text_box = container.find(\"div\",{\"class\":\"feed-shared-update-v2__description-wrapper ember-view\"})\n",
    "            text = text_box.find(\"span\",{\"dir\":\"ltr\"})\n",
    "            post_texts.append(text.text.strip())\n",
    "            post_names.append(name)\n",
    "            iterations += 1\n",
    "            print(iterations)\n",
    "            \n",
    "            if(iterations==nos):\n",
    "                break\n",
    "\n",
    "        except:\n",
    "            pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing scraped data into specified file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Name\": post_names,\n",
    "    \"Content\": post_texts,\n",
    "}\n",
    "\n",
    "# save to csv \n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"user_posts.csv\", encoding='utf-8', index=False)\n",
    "\n",
    "# save to excel \n",
    "writer = pd.ExcelWriter(\"user_posts.xlsx\", engine='xlsxwriter')\n",
    "df.to_excel(writer, index =False)\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bfb4883d108fc92ac768439090a2e92bb9a1f760a54beeecfd6762b5dcd70fe3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
